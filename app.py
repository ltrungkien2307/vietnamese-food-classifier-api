# ==========================================
# VIETNAMESE FOOD CLASSIFIER API - TF 2.18.0 Compatible
# ==========================================

from flask import Flask, request, jsonify
from flask_cors import CORS
import tensorflow as tf
import numpy as np
import pickle
from PIL import Image
import io
import base64
import os
from dotenv import load_dotenv
import logging

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global variables
model = None
class_names = None

# Vietnamese food names mapping
VIETNAMESE_NAMES = {
    'banh_beo': 'B√°nh B√®o',
    'banh_bot_loc': 'B√°nh B·ªôt L·ªçc', 
    'banh_can': 'B√°nh CƒÉn',
    'banh_canh': 'B√°nh Canh',
    'banh_chung': 'B√°nh Ch∆∞ng',
    'banh_cuon': 'B√°nh Cu·ªën',
    'banh_duc': 'B√°nh ƒê√∫c',
    'banh_gio': 'B√°nh Gi√≤',
    'banh_khot': 'B√°nh Kh·ªçt',
    'banh_mi': 'B√°nh M√¨',
    'banh_pia': 'B√°nh P√≠a',
    'banh_tet': 'B√°nh T√©t',
    'banh_trang_nuong': 'B√°nh Tr√°ng N∆∞·ªõng',
    'banh_xeo': 'B√°nh X√®o',
    'bun_bo_hue': 'B√∫n B√≤ Hu·∫ø',
    'bun_dau_mam_tom': 'B√∫n ƒê·∫≠u M·∫Øm T√¥m',
    'bun_mam': 'B√∫n M·∫Øm',
    'bun_rieu': 'B√∫n Ri√™u',
    'bun_thit_nuong': 'B√∫n Th·ªãt N∆∞·ªõng',
    'ca_kho_to': 'C√° Kho T·ªô',
    'canh_chua': 'Canh Chua',
    'cao_lau': 'Cao L·∫ßu',
    'chao_long': 'Ch√°o L√≤ng',
    'com_tam': 'C∆°m T·∫•m',
    'goi_cuon': 'G·ªèi Cu·ªën',
    'hu_tieu': 'H·ªß Ti·∫øu',
    'mi_quang': 'M√¨ Qu·∫£ng',
    'nem_chua': 'Nem Chua',
    'pho': 'Ph·ªü',
    'xoi_xeo': 'X√¥i X√©o'
}

# ==========================================
# TF 2.18.0 + KERAS 3.x COMPATIBLE MODEL LOADING
# ==========================================

def load_model_and_classes():
    """Load model compatible with TF 2.18.0 + Keras 3.x"""
    global model, class_names
    
    try:
        logger.info("üîÑ Loading model...")
        logger.info(f"üîß TensorFlow version: {tf.__version__}")
        
        model_path = os.getenv('MODEL_PATH', 'models/mobilenet_food_classifier.h5')
        class_names_path = os.getenv('CLASS_NAMES_PATH', 'models/vietnamese_food_class_names.pkl')
        
        # Check if files exist
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        # Load model with TF 2.18.0 compatibility
        logger.info(f"üìÇ Loading model from: {model_path}")
        
        # For TF 2.18.0, try direct loading first
        try:
            model = tf.keras.models.load_model(model_path)
            logger.info("‚úÖ Direct model loading successful!")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Direct loading failed: {str(e)}")
            
            # Try loading without compilation
            logger.info("üîß Trying to load without compilation...")
            model = tf.keras.models.load_model(model_path, compile=False)
            
            # Recompile with current TF version
            logger.info("üîß Recompiling model...")
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            logger.info("‚úÖ Model recompiled successfully!")
        
        # Load class names
        if os.path.exists(class_names_path):
            with open(class_names_path, 'rb') as f:
                class_names = pickle.load(f)
            logger.info(f"‚úÖ Class names loaded: {len(class_names)} classes")
        else:
            # Fallback to default class names
            class_names = list(VIETNAMESE_NAMES.keys())
            logger.warning("‚ö†Ô∏è Class names file not found, using default names")
        
        # Verify model
        logger.info(f"üéØ Model loaded successfully!")
        logger.info(f"   üìê Input shape: {model.input_shape}")
        logger.info(f"   üìê Output shape: {model.output_shape}")
        logger.info(f"   üè∑Ô∏è  Classes: {len(class_names)}")
        
        # Test prediction to ensure everything works
        logger.info("üß™ Testing model with dummy input...")
        dummy_input = np.random.random((1, 224, 224, 3)).astype(np.float32)
        test_pred = model.predict(dummy_input, verbose=0)
        logger.info(f"‚úÖ Model test successful! Output shape: {test_pred.shape}")
        
    except Exception as e:
        logger.error(f"‚ùå Error loading model: {str(e)}")
        raise e

# ==========================================
# IMAGE PROCESSING
# ==========================================

def preprocess_image(image_data, target_size=(224, 224)):
    """Preprocess image for model prediction"""
    try:
        # Decode base64 image
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Resize to target size
        image = image.resize(target_size)
        
        # Convert to numpy array and normalize
        image_array = np.array(image, dtype=np.float32) / 255.0
        
        # Add batch dimension
        image_array = np.expand_dims(image_array, axis=0)
        
        return image_array
        
    except Exception as e:
        logger.error(f"‚ùå Error preprocessing image: {str(e)}")
        raise e

# ==========================================
# PREDICTION
# ==========================================

def predict_food(image_array, top_k=5):
    """Make prediction on preprocessed image"""
    try:
        # Make prediction
        predictions = model.predict(image_array, verbose=0)
        
        # Handle different output shapes from Keras 3.x
        if len(predictions.shape) > 1:
            predictions = predictions[0]  # Remove batch dimension
        
        # Get top-k predictions
        top_indices = np.argsort(predictions)[::-1][:top_k]
        
        results = []
        for i, idx in enumerate(top_indices):
            if idx < len(class_names):  # Safety check
                class_name = class_names[idx]
                vietnamese_name = VIETNAMESE_NAMES.get(class_name, class_name.replace('_', ' ').title())
                confidence = float(predictions[idx])
                
                results.append({
                    'rank': i + 1,
                    'class_id': class_name,
                    'class_name': vietnamese_name,
                    'confidence': confidence,
                    'percentage': round(confidence * 100, 2)
                })
        
        return results
        
    except Exception as e:
        logger.error(f"‚ùå Error making prediction: {str(e)}")
        raise e

# ==========================================
# API ROUTES
# ==========================================

@app.route('/', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'message': 'üçú Vietnamese Food Classifier API',
        'model_loaded': model is not None,
        'classes_count': len(class_names) if class_names else 0,
        'tensorflow_version': tf.__version__,
        'keras_version': tf.keras.__version__ if hasattr(tf.keras, '__version__') else 'N/A'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Main prediction endpoint"""
    try:
        # Check if model is loaded
        if model is None:
            return jsonify({
                'error': 'Model not loaded',
                'message': 'Server is still loading the model. Please try again.'
            }), 503
        
        # Get request data
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({
                'error': 'No image provided',
                'message': 'Please provide image data in base64 format'
            }), 400
        
        # Preprocess image
        logger.info("üñºÔ∏è Processing image...")
        image_array = preprocess_image(data['image'])
        
        # Make prediction
        logger.info("ü§ñ Making prediction...")
        top_k = data.get('top_k', 5)
        predictions = predict_food(image_array, top_k)
        
        if not predictions:
            return jsonify({
                'error': 'No predictions generated',
                'message': 'Model could not generate predictions for this image'
            }), 500
        
        # Get best prediction
        best_prediction = predictions[0]
        
        logger.info(f"‚úÖ Prediction completed: {best_prediction['class_name']} ({best_prediction['percentage']:.1f}%)")
        
        return jsonify({
            'success': True,
            'best_prediction': {
                'food_name': best_prediction['class_name'],
                'confidence': best_prediction['confidence'],
                'percentage': best_prediction['percentage']
            },
            'top_predictions': predictions,
            'model_info': {
                'total_classes': len(class_names),
                'model_type': 'MobileNetV2 Transfer Learning',
                'tensorflow_version': tf.__version__,
                'keras_version': tf.keras.__version__ if hasattr(tf.keras, '__version__') else 'N/A'
            }
        })
        
    except Exception as e:
        logger.error(f"‚ùå Prediction error: {str(e)}")
        return jsonify({
            'error': 'Prediction failed',
            'message': str(e)
        }), 500

@app.route('/classes', methods=['GET'])
def get_classes():
    """Get all available food classes"""
    try:
        if class_names is None:
            return jsonify({
                'error': 'Classes not loaded'
            }), 503
        
        classes_info = []
        for i, class_name in enumerate(class_names):
            vietnamese_name = VIETNAMESE_NAMES.get(class_name, class_name.replace('_', ' ').title())
            classes_info.append({
                'id': i,
                'class_id': class_name,
                'vietnamese_name': vietnamese_name
            })
        
        return jsonify({
            'total_classes': len(class_names),
            'classes': classes_info
        })
        
    except Exception as e:
        logger.error(f"‚ùå Error getting classes: {str(e)}")
        return jsonify({
            'error': 'Failed to get classes',
            'message': str(e)
        }), 500

@app.route('/batch_predict', methods=['POST'])
def batch_predict():
    """Batch prediction for multiple images"""
    try:
        if model is None:
            return jsonify({'error': 'Model not loaded'}), 503
        
        data = request.get_json()
        images = data.get('images', [])
        
        if not images:
            return jsonify({'error': 'No images provided'}), 400
        
        if len(images) > 10:  # Limit batch size
            return jsonify({'error': 'Too many images. Maximum 10 per batch.'}), 400
        
        results = []
        
        for i, image_data in enumerate(images):
            try:
                image_array = preprocess_image(image_data)
                predictions = predict_food(image_array, top_k=3)
                
                results.append({
                    'image_index': i,
                    'success': True,
                    'predictions': predictions
                })
                
            except Exception as e:
                results.append({
                    'image_index': i,
                    'success': False,
                    'error': str(e)
                })
        
        return jsonify({
            'success': True,
            'total_images': len(images),
            'results': results
        })
        
    except Exception as e:
        logger.error(f"‚ùå Batch prediction error: {str(e)}")
        return jsonify({
            'error': 'Batch prediction failed',
            'message': str(e)
        }), 500

# ==========================================
# ERROR HANDLERS
# ==========================================

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        'error': 'Endpoint not found',
        'message': 'The requested endpoint does not exist'
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        'error': 'Internal server error',
        'message': 'Something went wrong on the server'
    }), 500

# ==========================================
# STARTUP
# ==========================================

def create_app():
    """Application factory"""
    
    # Load model on startup
    try:
        load_model_and_classes()
        logger.info("üöÄ API ready to serve requests!")
    except Exception as e:
        logger.error(f"‚ùå Failed to start API: {str(e)}")
        # Continue without model for health checks
    
    return app

# ==========================================
# MAIN
# ==========================================

if __name__ == '__main__':
    # Development server
    app = create_app()
    
    port = int(os.getenv('PORT', 5000))
    debug = os.getenv('DEBUG', 'False').lower() == 'true'
    
    logger.info(f"üåü Starting development server on port {port}")
    logger.info(f"üîß TensorFlow version: {tf.__version__}")
    app.run(host='0.0.0.0', port=port, debug=debug)